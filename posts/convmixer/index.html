<!doctype html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
   <link rel="stylesheet" href="/libs/katex/katex.min.css">
     
   <link rel="stylesheet" href="/libs/highlight/github.min.css">
   
  <link rel="stylesheet" href="/css/franklin.css">
<link rel="stylesheet" href="/css/poole_hyde.css">
<!-- style adjustments -->
<style>
  html {font-size: 17px;}
  .franklin-content {position: relative; padding-left: 8%; padding-right: 5%; line-height: 1.35em;}
  @media (min-width: 940px) {
    .franklin-content {width: 100%; margin-left: auto; margin-right: auto;}
  }
  @media (max-width: 768px) {
    .franklin-content {padding-left: 6%; padding-right: 6%;}
  }
</style>
<link rel="icon" href="/assets/lichen.png">

   <title>The ConvMixer architecture: 🤷</title> 

</head>
<body>
<div class="sidebar">
  <div class="container sidebar-sticky">

    <div class="sidebar-about">

      <h1><a class="sidebar-about" href="/">Simon Coste</a></h1>
      <p class="sidebar-about">Mathematics</p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item " href="/">About</a>
      <a class="sidebar-nav-item " href="/research/">Research</a>
      <a class="sidebar-nav-item " href="/teaching/">Teaching</a>
      <a class="sidebar-nav-item " href="/notes/">Notes</a>
      <a class="sidebar-nav-item " href="/talk/">Talks</a>
    </nav>

    <div class="sidebar-foot">
      <p>&copy; Simon Coste, modified: September 14, 2023. <br> Built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language. </a></p>
    </div>


  </div>
  

  
</div>
<div class="content container">

<!-- Content appended here -->


<div class="franklin-content">
   <h1 class="page-title"> The ConvMixer architecture: 🤷 </h1> 
   <span class="page-date"> December 2021 </span> 
</div>
<div class="franklin-content">
<p>In computer vision, <strong>Residual Networks</strong> &#40;ResNets&#41; are an important and successful architecture; beating the performance of deep ResNets on image classification tasks like ImageNet remains a noticeable achievement. In natural language processing, <strong>Transformers</strong> play the same role and are now widely used and studied, especially through Bert-like architectures. </p>
<p>Residual networks are mostly based on <em>convolutions</em>; that is, they transform images by applying local filters, and they combine these convolutions in various way. On the other hand, Transformers are based on <em>attention mechanisms</em>: they consider words in sentences as elementary units and learn which ones are the most important. Using attention mechanisms in vision successfully led to <strong>Vision Transformers</strong>, but it is not clear why they perform well; in trying to understand this, a few purely-convolutional architectures emerged, who reached very good performances while at the same time being really simpler than ResNets.  </p>
<p>The goal of this post is to show one of these architectures, <em>ConvMixer</em>, implemented with Julia&#39;s <a href="https://fluxml.ai/Flux.jl/stable/">Flux</a> machine learning library. </p>
<p><img src="/posts/img/logo.png" alt="flux" /></p>
<p><hr /> </p>
<div class="franklin-toc"><ol><li><a href="#overview_vision_transformers_and_the_role_of_patches">Overview: Vision Transformers and the role of patches</a></li><li><a href="#build_the_mixer">Build the mixer</a></li><li><a href="#train_the_mixer">Train the mixer </a></li><li><a href="#im_not_a_code_golfer">I&#39;m not a code golfer</a></li><li><a href="#julias_flux_library">Julia&#39;s Flux library</a></li><li><a href="#references_the_paper_title_competition">References: the Paper Title Competition</a></li></ol></div>
<hr />
<h2 id="overview_vision_transformers_and_the_role_of_patches"><a href="#overview_vision_transformers_and_the_role_of_patches" class="header-anchor">Overview: Vision Transformers and the role of patches</a></h2>
<p>Since 2017, it was widely discussed in natural language processing whether <a href="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">attention is all you need</a>. The term <strong>attention</strong> refers to learning the relations between different bits of the input. For text processing, the bits are words or sentences; attention mechanisms &#40;and also self-attention&#41; learn this, leading to what is called Transformers. </p>
<p>This idea was adapted to vision tasks. There are no words in pictures, but one can still split any picture in <strong>patches</strong> and treat them as words in a text, feeding them to Transformer architectures, hence the name <em>Vision Transformers</em> &#40;ViT&#41;. It works, and indeed <a href="https://arxiv.org/pdf/2010.11929.pdf">an image is worth 16x16 words</a>. But it is not completely clear if the success of these methods comes from these attention mechanisms: in other words, <a href="https://arxiv.org/pdf/2105.02723.pdf">do you even need attention?</a></p>
<p>It seems that splitting images in patches, then stacking classical networks applied to these patches is enough for very high performances. Indeed, another recent model, <code>ConvMixer</code>, also suggested that <a href="https://openreview.net/pdf?id&#61;TVHS5Y4dNvM">patches are all you need</a>, and that it is rather the patch representation of pictures which is responsible for this gain in performance. </p>
<p>The <code>ConvMixer</code> is really simple: it first splits the image in different patches, and then feeds it to a chain of convolutional networks alternatively applied channel-wise or space-wise. No recurrence or self-attention here, and yet this model reaches excellent performances&#33;</p>
<h2 id="build_the_mixer"><a href="#build_the_mixer" class="header-anchor">Build the mixer</a></h2>
<p>Let&#39;s build the architecture. All the basic blocks &#40;convolutions, nonlinearities, batch normalizations, mean pooling and dense layers&#41; are natively available in Flux, we only have to assemble them. The input of our architecture will be a batch of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi></mrow><annotation encoding="application/x-tex">B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span></span></span></span> images of size <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>32</mn><mo>×</mo><mn>32</mn></mrow><annotation encoding="application/x-tex">32 \times 32</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">3</span><span class="mord">2</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">3</span><span class="mord">2</span></span></span></span> with three &#40;RGB&#41; channels, hence the input dimension is </p>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mo stretchy="false">(</mo><mn>32</mn><mo separator="true">,</mo><mn>32</mn><mo separator="true">,</mo><mn>3</mn><mo separator="true">,</mo><mi>B</mi><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">(32, 32, 3, B).</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">3</span><span class="mord">2</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">3</span><span class="mord">2</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">3</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="mclose">)</span><span class="mord">.</span></span></span></span></span>
<p>The architecture of ConvMixer can be summarized in the following picture, taken from the paper: </p>
<p><img src="/posts/img/convmixerarchi.png" alt="" /></p>
<p>Let&#39;s decompose it block by block.</p>
<h3 id="chain"><a href="#chain" class="header-anchor">Chain</a></h3>
<p>In Flux, the basic way to compose layers is <code>Chain</code>, the equivalent of Torch&#39;s <code>nn.Sequential</code>. There&#39;s nothing more to say. </p>
<h3 id="patch_embedding_as_a_convolution"><a href="#patch_embedding_as_a_convolution" class="header-anchor">Patch embedding as a convolution</a></h3>
<p>The main point of the ConvMixer architecture is that it begins by splitting an image in patches of size <code>&#40;p, p&#41;</code>. This is done using the convolutional layer and the <code>stride</code> argument. Let us take a <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>p</mi><mo separator="true">,</mo><mi>p</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(p,p)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">p</span><span class="mclose">)</span></span></span></span> convolution. If <code>stride&#61;1</code>, the convolution is applied around every pixel in the image. If <code>stride &#61; 2</code>, it is applied only on one over two pixels, etc. With <code>stride&#61;p</code>, the windows on which the convolution is applied are all disjoint and adjacent, thus covering the image in <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>p</mi><mo separator="true">,</mo><mi>p</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(p,p)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">p</span><span class="mclose">)</span></span></span></span> patches. This is why the first layer of ConvMixer is </p>
<pre><code class="julia hljs">Conv((p, p), <span class="hljs-number">3</span>=&gt;H, gelu; stride=p).</code></pre>
<p>In Flux, the third argument of a convolution, if specified, is an activation function applied pointwise<sup id="fnref:1"><a href="#fndef:1" class="fnref">[1]</a></sup>. Here, we take the <a href="https://paperswithcode.com/method/gelu">Gaussian error linear unit</a>, Gelu. Additionnaly, we transform the 3 initial RGB channels in <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi></mrow><annotation encoding="application/x-tex">H</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span></span></span></span> &#40;like <em>hidden</em>&#41; dimensions. After this operation, the batch has dimension</p>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mo stretchy="false">(</mo><mn>32</mn><mi mathvariant="normal">/</mi><mi>p</mi><mo separator="true">,</mo><mn>32</mn><mi mathvariant="normal">/</mi><mi>p</mi><mo separator="true">,</mo><mi>H</mi><mo separator="true">,</mo><mi>B</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(32/p, 32/p, H, B)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">3</span><span class="mord">2</span><span class="mord">/</span><span class="mord mathnormal">p</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">3</span><span class="mord">2</span><span class="mord">/</span><span class="mord mathnormal">p</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="mclose">)</span></span></span></span></span>
<h3 id="batchnorm"><a href="#batchnorm" class="header-anchor">Batchnorm</a></h3>
<p>In ConvMixer as in many networks, each layer is followed by a batch normalization: after being filtered, the arrays representing images in a batch are centered and reduced along each dimension. There are two learnable parameters &#40;mean and std&#41; for each dimension. In Flux, we simply call <code>BatchNorm&#40;H&#41;</code>. </p>
<h3 id="residual_networks"><a href="#residual_networks" class="header-anchor">Residual networks</a></h3>
<p>The next ingredient for the ConvMixer architecture is the <em>residual connection</em>: instead of filtering <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">x</span></span></span></span> to <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span> and feeding <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span> to the next layer, we feed <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>+</mo><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">x+f(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span> to the next layer. In Flux, this is done with <code>SkipConnection&#40;layer, &#43;&#41;</code> where <code>layer</code> is any chain of layers. Note that we are not forced to use addition <code>&#43;</code> to perform the connection. We could multiply, concatenate or anything else.  </p>
<p>We use these SkipConnections with layers composed of channel-wise convolutions: here the <code>groups</code> argument tells us that the convolution is applied indepedently on each channel.  </p>
<pre><code class="julia hljs">SkipConnection(
    Chain(
        Conv((kernel,kernel), H=&gt;H, gelu; pad=SamePad(), groups=H),
        BatchNorm(H)), 
    +
)</code></pre>
<p>This residual layer is followed by a purely pixel-wise convolutional layer, and this whole operation is repeated <code>depth</code> times. </p>
<h3 id="last_layers"><a href="#last_layers" class="header-anchor">Last layers</a></h3>
<p>At the end of the last convolutional layer, we have a batch of dimension <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mn>32</mn><mi mathvariant="normal">/</mi><mi>p</mi><mo separator="true">,</mo><mn>32</mn><mi mathvariant="normal">/</mi><mi>p</mi><mo separator="true">,</mo><mi>H</mi><mo separator="true">,</mo><mi>B</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(32/p, 32/p, H, B)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">3</span><span class="mord">2</span><span class="mord">/</span><span class="mord mathnormal">p</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">3</span><span class="mord">2</span><span class="mord">/</span><span class="mord mathnormal">p</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="mclose">)</span></span></span></span>. It is time to reduce dimensions: we average over each channel with <code>AdaptativeMeanPool&#40;&#40;1,1&#41;&#41;</code> to obtain a <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mn>1</mn><mo separator="true">,</mo><mn>1</mn><mo separator="true">,</mo><mi>H</mi><mo separator="true">,</mo><mi>B</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(1,1,H, B)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="mclose">)</span></span></span></span> batch, then we apply a dense layer &#40;which is only applied to flattened arrays, hence the <code>flatten</code> layer right before&#41;: </p>
<pre><code class="julia hljs">Chain(AdaptiveMeanPool((<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)), flatten, Dense(H,N_classes))</code></pre>
<h3 id="the_final_implementation"><a href="#the_final_implementation" class="header-anchor">The final implementation</a></h3>
<pre><code class="julia hljs"><span class="hljs-keyword">using</span> Flux

<span class="hljs-keyword">function</span> ConvMixer(in_channels, H, k, patch, depth, N_classes)
    <span class="hljs-keyword">return</span> Chain(
            Conv((patch, patch), in_channels=&gt;H, gelu; stride=patch),
            BatchNorm(H),
            [
                Chain(
                    SkipConnection(
                        Chain(
                            Conv((k,k),H=&gt;H,gelu; pad=SamePad(), groups=H), 
                            BatchNorm(H)
                        ), 
                    +),
                    Chain(Conv((<span class="hljs-number">1</span>,<span class="hljs-number">1</span>), H=&gt;H, gelu), BatchNorm(H))
                ) 
                <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-number">1</span>:depth
            ]...,
            AdaptiveMeanPool((<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)),
            flatten,
            Dense(H,N_classes)
        )
    <span class="hljs-keyword">return</span> f
<span class="hljs-keyword">end</span></code></pre>
<p>Let&#39;s compute the number of parameters: the patch-splitting convolution has <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn><mo>×</mo><mi>H</mi><mo>×</mo><msup><mi>p</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">3 \times H\times p^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.008548em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span> parameters. The BatchNorm layer has <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mi>H</mi></mrow><annotation encoding="application/x-tex">2H</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord">2</span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span></span></span></span> parameters. Each of the <code>depth</code> layers has <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mo>×</mo><msup><mi>k</mi><mn>2</mn></msup><mo>+</mo><mn>2</mn><mi>H</mi><mo>+</mo><msup><mi>H</mi><mn>2</mn></msup><mo>+</mo><mn>2</mn><mi>H</mi></mrow><annotation encoding="application/x-tex">H\times k^2 + 2H + H^2 + 2H</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.897438em;vertical-align:-0.08333em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord">2</span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.897438em;vertical-align:-0.08333em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord">2</span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span></span></span></span> parameters &#40;conv, batchnorm, conv, batchnorm&#41;. The last layer is affine, hence it has <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mo>×</mo><mi>N</mi><mo>+</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">H \times N + N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span> parameters. Overall we have</p>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="script">P</mi><mo stretchy="false">(</mo><mi>k</mi><mo separator="true">,</mo><mi>p</mi><mo separator="true">,</mo><mi>H</mi><mo separator="true">,</mo><mi>N</mi><mo separator="true">,</mo><mtext>depth</mtext><mo stretchy="false">)</mo><mo>=</mo><mn>3</mn><mi>H</mi><msup><mi>p</mi><mn>2</mn></msup><mo>+</mo><mn>2</mn><mi>H</mi><mo>+</mo><mtext>depth</mtext><mo stretchy="false">(</mo><mi>H</mi><msup><mi>k</mi><mn>2</mn></msup><mo>+</mo><mn>4</mn><mi>H</mi><mo>+</mo><msup><mi>H</mi><mn>2</mn></msup><mo stretchy="false">)</mo><mo>+</mo><mi>N</mi><mi>H</mi><mo>+</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">\mathscr{P}(k,p,H,N, \text{depth}) = 3 H p^2 + 2H + \text{depth}(Hk^2 + 4H + H^2) + NH + N </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathscr" style="margin-right:0.08078em;">P</span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">p</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord text"><span class="mord">depth</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.0585479999999998em;vertical-align:-0.19444em;"></span><span class="mord">3</span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord">2</span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.1141079999999999em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">depth</span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord">4</span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.1141079999999999em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span></span>
<p>learnable parameters in this architecture<sup id="fnref:2"><a href="#fndef:2" class="fnref">[2]</a></sup>. </p>
<h2 id="train_the_mixer"><a href="#train_the_mixer" class="header-anchor">Train the mixer </a></h2>
<h3 id="the_cifar10_dataset"><a href="#the_cifar10_dataset" class="header-anchor">The Cifar10 Dataset</a></h3>
<p>Training large networks on the reference ImageNet dataset is resource-consuming. I&#39;m sticking to the smaller dataset CIFAR10. </p>
<pre><code class="julia hljs"><span class="hljs-keyword">using</span> MLDatasets
<span class="hljs-keyword">using</span> Flux:onehotbatch, Dataloader

<span class="hljs-keyword">function</span> get_CIFAR_data(batchsize; idxs = <span class="hljs-literal">nothing</span>)
    <span class="hljs-string">&quot;&quot;&quot;
        idxs=nothing gives the full dataset.
        otherwise only the 1:idxs elements of the train set are given.
    &quot;&quot;&quot;</span>
    <span class="hljs-literal">ENV</span>[<span class="hljs-string">&quot;DATADEPS_ALWAYS_ACCEPT&quot;</span>] = <span class="hljs-string">&quot;true&quot;</span> 

    <span class="hljs-keyword">if</span> idxs==<span class="hljs-literal">nothing</span>
        xtrain, ytrain = MLDatasets.CIFAR.traindata(<span class="hljs-built_in">Float32</span>)
        xtest, ytest = MLDatasets.CIFAR.testdata(<span class="hljs-built_in">Float32</span>)
	<span class="hljs-keyword">else</span>
        xtrain, ytrain = MLDatasets.CIFAR.traindata(<span class="hljs-built_in">Float32</span>, <span class="hljs-number">1</span>:idxs)
        xtest, ytest = MLDatasets.CIFAR.testdata(<span class="hljs-built_in">Float32</span>, <span class="hljs-number">1</span>:<span class="hljs-built_in">Int</span>(idxs/<span class="hljs-number">10</span>))
    <span class="hljs-keyword">end</span>

    <span class="hljs-comment"># Reshape Data to comply to Julia&#x27;s convention:</span>
    <span class="hljs-comment">#(width, height, channels, batch_size)</span>
    xtrain = reshape(xtrain, (<span class="hljs-number">32</span>,<span class="hljs-number">32</span>,<span class="hljs-number">3</span>,:))
    xtest = reshape(xtest, (<span class="hljs-number">32</span>,<span class="hljs-number">32</span>,<span class="hljs-number">3</span>,:))
    ytrain, ytest = onehotbatch(ytrain, <span class="hljs-number">0</span>:<span class="hljs-number">9</span>), onehotbatch(ytest, <span class="hljs-number">0</span>:<span class="hljs-number">9</span>)

    train_loader = DataLoader((xtrain, ytrain), batchsize, shuffle=<span class="hljs-literal">true</span>)
    test_loader = DataLoader((xtest, ytest), batchsize)

    <span class="hljs-keyword">return</span> train_loader, test_loader
<span class="hljs-keyword">end</span></code></pre>
<p>Flux&#39;s <code>Dataloader</code> splits the data in batches of the given size, possibly shuffled, and returns an iterable object whose elements are <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(x,y)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mclose">)</span></span></span></span> where <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">x</span></span></span></span> is a batch and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span></span></span> the corresponding labels.  </p>
<p>Also, we shouldn&#39;t train our model on raw CIFAR10; we should augment it using classical procedures &#40;random modifications, mixups and so on&#41;. I&#39;ll do this next time using <code>Augmentor.jl</code>.</p>
<h3 id="gpu_support"><a href="#gpu_support" class="header-anchor">GPU support</a></h3>
<p>Flux uses the CUDA toolbox: the <code>device&#40;T&#41;</code> method takes any object <code>T</code> and puts it on <code>device</code>. If you have a batch of images and labels <code>x,y</code> drawn from your dataloader, you put it on the gpu using <code>x &#61; gpu&#40;x&#41;</code> and back on the cpu with <code>x &#61; cpu&#40;x&#41;</code>.</p>
<h3 id="cross-entropy_loss_and_classification_accuracy"><a href="#cross-entropy_loss_and_classification_accuracy" class="header-anchor">Cross-entropy loss and classification accuracy</a></h3>
<p>Since we deal with a classification task &#40;there are 10 classes on CIFAR&#41;, we&#39;ll train the network using the logit cross entropy. Just for the sake of writing maths, let&#39;s recall how it works. With <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mo>=</mo><mn>10</mn></mrow><annotation encoding="application/x-tex">C = 10</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord">0</span></span></span></span> classes, our architecture is fed an image <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">x_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>, and outputs the probability <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover accent="true"><mi>p</mi><mo>^</mo></mover><mrow><mi>i</mi><mo separator="true">,</mo><mi>c</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\hat{p}_{i, c}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.980548em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">p</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.16666em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">c</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> that this image belongs to class <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi></mrow><annotation encoding="application/x-tex">c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">c</span></span></span></span>. The ground truth would be <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>c</mi></mrow></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">p_{i,c}=0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.716668em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">c</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span></span></span></span> for all the classes <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi></mrow><annotation encoding="application/x-tex">c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">c</span></span></span></span> except for the <em>real</em> class <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi></mrow><annotation encoding="application/x-tex">c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">c</span></span></span></span> of image <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">x_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>, for which <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>c</mi></mrow></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">p_{i,c}=1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.716668em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">c</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span>. The cross-entropy &#40;<code>Flux.logitcrossentropy</code>&#41; measures this discrepancy in the following way:</p>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>L</mi><mo stretchy="false">(</mo><msub><mover accent="true"><mi>p</mi><mo>^</mo></mover><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>p</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><munderover><mo>∑</mo><mrow><mi>c</mi><mo>=</mo><mn>1</mn></mrow><mi>C</mi></munderover><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mover accent="true"><mi>p</mi><mo>^</mo></mover><mrow><mi>i</mi><mo separator="true">,</mo><mi>c</mi></mrow></msub><mo stretchy="false">)</mo><msub><mi>p</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>c</mi></mrow></msub><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex"> L(\hat{p}_i, p_i) = - \sum_{c=1}^C \log(\hat{p}_{i,c})p_{i,c}.</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">p</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.16666em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:3.0954490000000003em;vertical-align:-1.267113em;"></span><span class="mord">−</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283360000000002em;"><span style="top:-1.882887em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">c</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.050005em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3000050000000005em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">C</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.267113em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mopen">(</span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">p</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.16666em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">c</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">c</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mord">.</span></span></span></span></span>
<p>We&#39;ll also keep track of the accuracy of the model: since we want to predict classes, and not only probabilities, we predict the class of an image to be the one with the highest predicted probability <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover accent="true"><mi>p</mi><mo>^</mo></mover><mrow><mi>i</mi><mo separator="true">,</mo><mi>c</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\hat{p}_{i,c}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.980548em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">p</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.16666em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">c</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span>. That&#39;s what the <code>onecold</code> function from Flux does. The proportion of correct predictions is the accuracy. </p>
<pre><code class="julia hljs"><span class="hljs-keyword">using</span> Flux:onecold, logitcrossentropy

<span class="hljs-keyword">function</span> ℓ(dataloader, model, device)
    <span class="hljs-string">&quot;&quot;&quot;batch-wise loss and accuracy&quot;&quot;&quot;</span>
    
    n = <span class="hljs-number">0</span>
    cross_entropy = <span class="hljs-number">0.0f0</span>
    accuracy = <span class="hljs-number">0.0f0</span>

    <span class="hljs-keyword">for</span> (x,y) <span class="hljs-keyword">in</span> dataloader
        x,y = x |&gt; device, y |&gt; device
        z = model(x)        
        cross_entropy += logitcrossentropy(z, y, agg=sum)
        accuracy += sum(onecold(z).==onecold(y))
        n += size(x)[<span class="hljs-keyword">end</span>]
    <span class="hljs-keyword">end</span>
    cross_entropy / n, accuracy / n
<span class="hljs-keyword">end</span></code></pre>
<h3 id="training_loop"><a href="#training_loop" class="header-anchor">Training loop</a></h3>
<p>Let&#39;s put everything together. </p>
<pre><code class="julia hljs"><span class="hljs-keyword">using</span> Flux:Optimiser
<span class="hljs-keyword">using</span> BSON:<span class="hljs-meta">@save</span>

<span class="hljs-keyword">function</span> train(n_epochs=<span class="hljs-number">200</span>, η=<span class="hljs-number">3e-4</span>, device=gpu)

    train_loader, test_loader = get_data(<span class="hljs-number">128</span>)
    patch_size = <span class="hljs-number">2</span>
    kernel_size = <span class="hljs-number">7</span>
    dim = <span class="hljs-number">128</span>
    depth = <span class="hljs-number">8</span>

    <span class="hljs-comment">#for saving the losses and accuracy</span>
    train_save = zeros(n_epochs, <span class="hljs-number">2</span>)
    test_save = zeros(n_epochs, <span class="hljs-number">2</span>)

    model = ConvMixer(<span class="hljs-number">3</span>, kernel_size, patch_size, dim, depth, <span class="hljs-number">10</span>) |&gt; device

    ps = params(model)
    opt = Optimiser(
            WeightDecay(<span class="hljs-number">1f-3</span>), 
            ClipNorm(<span class="hljs-number">1.0</span>),
            ADAM(η)
            )

    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-number">1</span>:n_epochs
        <span class="hljs-keyword">for</span> (x,y) <span class="hljs-keyword">in</span> train_loader
            x,y = x|&gt;device, y|&gt;device
            gr = gradient(()-&gt;Flux.logitcrossentropy(model(x), y, agg=sum), ps)
            Flux.Optimise.update!(opt, ps, gr)
        <span class="hljs-keyword">end</span>

        <span class="hljs-comment">#logging</span>
        train_loss, train_acc = ℓ(train_loader, model, device) |&gt; cpu
        test_loss, test_acc = ℓ(test_loader, model, device) |&gt; cpu
        train_save[epoch,:] = [train_loss, train_acc]
        test_save[epoch,:] = [test_loss, test_acc]

        <span class="hljs-keyword">if</span> epoch%<span class="hljs-number">5</span>==<span class="hljs-number">0</span>
            <span class="hljs-meta">@info</span> <span class="hljs-string">&quot;t=<span class="hljs-variable">$epoch</span> : Train loss = <span class="hljs-variable">$train_loss</span> | Test acc. = <span class="hljs-variable">$test_acc</span>.&quot;</span>
        <span class="hljs-keyword">end</span>

    <span class="hljs-keyword">end</span>

    model = model |&gt; cpu
    <span class="hljs-meta">@save</span> <span class="hljs-string">&quot;save/model.bson&quot;</span> model 
    <span class="hljs-meta">@save</span> <span class="hljs-string">&quot;save/losses.bson&quot;</span> train_save test_save
<span class="hljs-keyword">end</span></code></pre>
<p>Some comments:</p>
<ol>
<li><p>Patches are small &#40;<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>×</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">2 \times 2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">2</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span></span></span></span>&#41;. For small datasets like CIFAR, we don&#39;t need large patches and indeed the best results are obtained with <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">p=1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">p</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span>. However, for experiments on larger datasets like ImageNet where pictures are <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>224</mn><mo>×</mo><mn>224</mn></mrow><annotation encoding="application/x-tex">224\times 224</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">2</span><span class="mord">2</span><span class="mord">4</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span><span class="mord">2</span><span class="mord">4</span></span></span></span>, patches of sizes 7,8 or 9 perform well &#40;all this according to the ConvMixer paper&#41;.</p>
</li>
</ol>
<ol start="2">
<li><p>The optimiser includes weight decay <em>and</em> gradient clipping with naive parameters. </p>
</li>
<li><p>The parameters were mostly taken from the ConvMixer paper; I chose them small enough so that the training of this model roughly took one afternoon on a Tesla P100.</p>
</li>
</ol>
<ol start="4">
<li><p>This network has less than 200k parameters, which is quite tiny. </p>
</li>
</ol>
<h3 id="training_curves"><a href="#training_curves" class="header-anchor">Training curves</a></h3>
<p>Most of the training time was spent overfitting with few improvement in the generalization error. </p>
<p><img src="/posts/img/fig.png" alt="" /></p>
<p>After 200 training epochs on batches of size 128, I got a labelling accuracy of 74&#37;. <strong>That&#39;s bad</strong>, but I underoptimised nearly everything: no augmentations on the dataset, no optimiser parameter scheduler, and a pretty small network.  For reference, the best performances on CIFAR are above 95&#37;; the best ConvMixer performance seems close to 97&#37; &#40;with 1.3 million parameters&#41;. </p>
<h2 id="im_not_a_code_golfer"><a href="#im_not_a_code_golfer" class="header-anchor">I&#39;m not a code golfer</a></h2>
<p>The authors of the ConvMixer paper argue that their architecture is as powerful as ResNets or ConViT high-performance models without any hyperparameter tuning, but the conceptual complexity of the model is considerably simpler: in fact, it is simplistic enough to fit in one tweet, ie less than 280 characters when implemented with the PyTorch framework. </p>
<p><img src="/posts/img/convmixer_singletweet.png " alt="a single tweet" /></p>
<p>This implies a little bit of <a href="https://en.wikipedia.org/wiki/Code_golf">code golf</a> cheating. In Julia, you can fit everything in a single tweet, <code>using Flux</code> included, in 272 characters without even needing to golf your functions names: </p>
<p><img src="/posts/img/carbon.png " alt="even better" /></p>
<p>The code is here &#40;with spaces&#41;:</p>
<blockquote>
<code>ConvMixer&#40;k,p,h,N&#41; &#61; Chain&#40;Conv&#40;&#40;p,p&#41;, 3&#61;&gt;h, gelu;stride&#61;p&#41;, BatchNorm&#40;h&#41;, &#91;Chain&#40;SkipConnection&#40;Chain&#40;Conv&#40;&#40;k,k&#41;,h&#61;&gt;h,gelu;pad&#61;SamePad&#40;&#41;,groups&#61;1&#41;, Batchnorm&#40;h&#41;&#41;,&#43;&#41;, Chain&#40;Conv&#40;&#40;1,1&#41;,h&#61;&gt;h,gelu&#41;, BatchNorm&#40;h&#41;&#41;&#41; for i in 1:D&#93;..., AdaptiveMeanPool&#40;&#40;1,1&#41;&#41;, flatten, Dense&#40;h,N&#41;&#41;</code>
</blockquote>
<h2 id="julias_flux_library"><a href="#julias_flux_library" class="header-anchor">Julia&#39;s Flux library</a></h2>
<p>The main goal of this post was to give a serious try to Julia&#39;s Flux library for deep learning. In short, it works. All the needed functionalities are here: the basic building blocks of deep vison architectures &#40;convolutions, residuals, batchnorms, nonlinearities, LSTM, RNN, etc.&#41; - I do not know if it&#39;s the same for NLP tools of GNNs. Autodiff works well and you can easily write your own gradients using the <code>@adjoint</code> macro. </p>
<p>Using a single GPU for ML experiments is just as easy as in any other framework. GPU computing works well with CUDA.jl<sup id="fnref:3"><a href="#fndef:3" class="fnref">[3]</a></sup>. I stumbled on bugs but the <a href="https://juliagpu.org/">JuliaGPU team</a> and <a href="https://github.com/maleadt">@maleadt &#40;Tim Besard&#41;</a> solved them at godspeed. </p>
<p>Last but not least, Flux benefits from the high flexibility and readability of the Julia language.   </p>
<p>In conclusion, there are many frameworks for doing deepl learning research right now: PyTorch, TensorFlow and satellites &#40;MXNet, Sonnet&#41;, Keras, JAX, Flux, Matlab... I&#39;m not a fan of communities wars, but it&#39;s fair to say that overall, PyTorch won: it&#39;s more flexible, it has the largest community, it&#39;s more robust and stable. But just as Torch or TF or Keras, <strong>Flux is also excellent</strong>. It has everything we need. It&#39;s sufficiently powerful, well-documented and flexible to be used in a day-to-day basis for Machine Learning research, and most importantly, <strong>to focus on content rather than on implementation</strong>. All hail Flux&#33; </p>
<h2 id="references_the_paper_title_competition"><a href="#references_the_paper_title_competition" class="header-anchor">References: the Paper Title Competition</a></h2>
<p><a href="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">Attention is all you need</a> was published 4 years ago and has 33k citations. </p>
<p><a href="https://arxiv.org/pdf/2010.11929.pdf">An image is worth 16x16 words</a> was written in 2020. </p>
<p>The first paper emphasizing the role of patches seems to be <a href="https://arxiv.org/pdf/2105.02723.pdf">Do you even need attention?</a>. </p>
<p>The original ConvMixer paper is <a href="https://openreview.net/pdf?id&#61;TVHS5Y4dNvM">Patches are all you need?</a> - the style is quite unconventional for a conference paper 🤷. </p>
<p>Most of my implementation of the training loop is inspired by the excellent <a href="https://github.com/FluxML/model-zoo">Flux Model Zoo</a>, a collection of simple implementations of classical deep learning architectures. </p>
<h3 id="notes"><a href="#notes" class="header-anchor">Notes</a></h3>
<p><sup id="fnref:1"><a href="#fndef:1" class="fnref">[1]</a></sup> In Torch, you would use <code>nn.Sequential&#40;nn.Conv2d&#40;...&#41;, nn.GeLU&#40;&#41;&#41;</code>. </p>
<p><sup id="fnref:2"><a href="#fndef:2" class="fnref">[2]</a></sup> The Convmixer paper mentions a slightly different formula; I&#39;m under the impression that their Batchnorm layers have <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn></mrow><annotation encoding="application/x-tex">3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">3</span></span></span></span> learnable parameters per channel. Where am I wrong?</p>
<p><sup id="fnref:3"><a href="#fndef:3" class="fnref">[3]</a></sup> Unfortunately, people having a non-NVIDIA GPU will have a hard time using it, and not only with JuliaGPU. Nvidia definitely took the high ground in the GPU game for machine learning.</p>
<!-- 
  Page footer is in the sidebar. 
 --></div><!-- CONTENT ENDS HERE -->
    </div>  <!-- div: content container -->
    
        



    
    
        


    
  </body>
</html>
